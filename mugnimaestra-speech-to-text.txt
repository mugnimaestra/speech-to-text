Directory structure:
└── mugnimaestra-speech-to-text/
    ├── README.md
    ├── eslint.config.mjs
    ├── lemonfoxapi.json
    ├── next.config.js
    ├── package.json
    ├── postcss.config.js
    ├── render.yaml
    ├── tailwind.config.ts
    ├── tsconfig.json
    ├── .dockerignore
    ├── .editorconfig
    ├── .yarnrc.yml
    ├── public/
    └── src/
        ├── app/
        │   ├── globals.css
        │   ├── layout.tsx
        │   ├── page.tsx
        │   └── api/
        │       ├── transcribe/
        │       │   └── route.ts
        │       ├── transcription-callback/
        │       │   └── route.ts
        │       └── transcription-status/
        │           └── [resultId]/
        │               └── route.ts
        ├── components/
        │   ├── LanguageSelector.tsx
        │   ├── AccessForm/
        │   │   └── index.tsx
        │   ├── AccessGate/
        │   │   └── index.tsx
        │   ├── SpeechToText/
        │   │   ├── FileDropZone.tsx
        │   │   ├── TranscriptionResult.tsx
        │   │   ├── URLInput.tsx
        │   │   ├── index.tsx
        │   │   └── types.ts
        │   └── Toast/
        │       └── index.tsx
        ├── hooks/
        │   └── useSpeechToText.ts
        └── lib/
            ├── authUtils.ts
            ├── constants.ts
            ├── transcriptionService.ts
            └── transcriptionStore.ts

================================================
File: README.md
================================================
# Media Transcription Service

A Next.js application that transcribes audio and video files to text using Lemonfox.ai's advanced transcription API.

## Features

- 🎵 Support for multiple audio formats: MP3, WAV, FLAC, OPUS, OGG, M4A
- 🎥 Support for video formats: MP4, MPEG, MOV, WEBM
- 📁 File size limits:
  - Direct upload: up to 100MB
  - URL: up to 1GB
- 🌐 Remote file transcription via URL
- 🎯 High accuracy transcription powered by Lemonfox.ai
- 📋 Easy copy-to-clipboard functionality
- 🖥️ Modern, responsive UI with drag-and-drop support

## Prerequisites

- Node.js 18 or later
- Yarn 4
- Lemonfox.ai API key

## Setup

1. Clone the repository:

```bash
git clone [repository-url]
cd speech-to-text
```

2. Install dependencies:

```bash
yarn install
```

3. Create a `.env.local` file in the root directory:

```bash
LEMONFOX_API_KEY=your_lemonfox_api_key_here
NEXT_PUBLIC_ACCESS_CODE=your_access_code_here  # Access code for authentication
```

To get your Lemonfox.ai API key:

1. Create an account at [Lemonfox.ai](https://lemonfox.ai)
2. Navigate to the API section in your dashboard
3. Generate a new API key
4. Copy the key to your `.env.local` file

## Development

Run the development server:

```bash
yarn dev
```

Open [http://localhost:3000](http://localhost:3000) to view the application.

## Usage

1. **Direct File Upload**

   - Drag and drop an audio/video file into the upload area
   - Or click to open the file selector
   - Supported file types: MP3, WAV, FLAC, OPUS, OGG, M4A, MP4, MPEG, MOV, WEBM
   - Maximum file size: 100MB
   - For larger files, please use URL upload

2. **URL Transcription**

   - Paste a URL to an audio/video file in the URL input field
   - Click "Transcribe URL"
   - Maximum file size: 1GB
   - URL must point directly to a media file

3. **View and Copy Results**
   - Transcription appears below the upload area
   - Use the "Copy to Clipboard" button to copy the text
   - Preview uploaded media files in the browser

## Deployment

### Railway.app Deployment

The application is optimized for deployment on Railway.app with the following features:

1. **File Size Support**
   - Direct file uploads up to 100MB
   - URL-based uploads up to 1GB
   - No timeout issues for long transcriptions (up to 1 hour)

2. **Deployment Steps**
   1. Push your code to GitHub/GitLab
   2. Create an account on [Railway.app](https://railway.app)
   3. Install Railway CLI:
      ```bash
      npm i -g @railway/cli
      ```
   4. Login to Railway:
      ```bash
      railway login
      ```
   5. Initialize Railway project in your local directory:
      ```bash
      railway init
      ```
      This will create a new project and link your directory to it.
   
   6. Add environment variables:
      ```bash
      railway variables --set LEMONFOX_API_KEY=your_api_key_here
      railway variables --set NEXT_PUBLIC_ACCESS_CODE=your_access_code_here
      ```
   
   7. Deploy your application:
      ```bash
      railway up
      ```
   
   8. Open your deployed application:
      ```bash
      railway open
      ```

Railway will automatically:
- Detect your Next.js application
- Install dependencies
- Build the application
- Deploy it with the proper configuration

Note: You can manage your environment variables using these commands:
```bash
railway variables                # List all variables
railway variables --unset KEY    # Remove a variable
railway run yarn dev            # Run locally with Railway variables
railway shell                  # Open a shell with Railway variables
```

## Error Handling

The application handles various error cases:

- Invalid file formats
- File size limits
- Invalid URLs
- API errors
- Network issues

Error messages are displayed clearly to the user with suggestions for resolution.

## Technical Details

- Framework: Next.js 14 with App Router
- Styling: Tailwind CSS
- API Integration: Lemonfox.ai Transcription API
- File Handling: Native File API + FormData
- Language: TypeScript

## Component Usage

```tsx
import SpeechToText from "@/components/SpeechToText";

// Basic usage
<SpeechToText />

// With handlers
<SpeechToText
  onTranscriptionComplete={(result) => console.log(result)}
  onError={(error) => console.error(error)}
/>
```

## Environment Variables

| Variable                | Description                                          | Required |
| ---------------------- | ---------------------------------------------------- | -------- |
| `LEMONFOX_API_KEY`     | Your Lemonfox.ai API key                            | Yes      |
| `NEXT_PUBLIC_ACCESS_CODE` | Access code for application authentication          | Yes      |

## API Routes

### POST /api/transcribe

Handles both direct file uploads and URL transcription requests.

Request body (FormData):
- `file`: File object or URL string

Response:
```json
{
  "text": "Transcribed text content",
  "segments": [
    {
      "id": 1,
      "text": "Segment text",
      "start": 0.0,
      "end": 1.0,
      "speaker": "A"
    }
  ]
}
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.


================================================
File: eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;


================================================
File: lemonfoxapi.json
================================================
{
    "openapi": "3.0.0",
    "info": {
      "title": "Lemonfox Speech-to-Text API",
      "description": "Convert audio to text quickly and reliably using Whisper v3, with support for speaker diarization and multiple languages.",
      "version": "1.0.0"
    },
    "servers": [
      {
        "url": "https://api.lemonfox.ai/v1"
      }
    ],
    "paths": {
      "/audio/transcriptions": {
        "post": {
          "summary": "Transcribe audio to text",
          "description": "Convert audio file to text with optional speaker diarization and various output formats",
          "requestBody": {
            "required": true,
            "content": {
              "multipart/form-data": {
                "schema": {
                  "type": "object",
                  "required": ["file"],
                  "properties": {
                    "file": {
                      "type": "string",
                      "format": "binary",
                      "description": "Audio file to transcribe (max 100MB) or URL to audio file (max 1GB)"
                    },
                    "response_format": {
                      "type": "string",
                      "enum": ["json", "text", "srt", "verbose_json", "vtt"],
                      "default": "json",
                      "description": "The format of the transcription response"
                    },
                    "speaker_labels": {
                      "type": "boolean",
                      "description": "Enable speaker diarization"
                    },
                    "min_speakers": {
                      "type": "integer",
                      "description": "Minimum number of speakers for diarization"
                    },
                    "max_speakers": {
                      "type": "integer",
                      "description": "Maximum number of speakers for diarization"
                    },
                    "prompt": {
                      "type": "string",
                      "description": "Optional text prompt to guide the transcription"
                    },
                    "language": {
                      "type": "string",
                      "description": "Language of the input audio"
                    },
                    "callback_url": {
                      "type": "string",
                      "format": "uri",
                      "description": "URL for receiving transcription results asynchronously"
                    },
                    "translate": {
                      "type": "boolean",
                      "description": "Translate the audio content to English"
                    },
                    "timestamp_granularities": {
                      "type": "array",
                      "items": {
                        "type": "string",
                        "enum": ["word"]
                      },
                      "description": "Enable word-level timestamps"
                    }
                  }
                }
              }
            }
          },
          "responses": {
            "200": {
              "description": "Successful transcription",
              "content": {
                "application/json": {
                  "schema": {
                    "type": "object",
                    "properties": {
                      "task": {
                        "type": "string",
                        "description": "Type of task performed"
                      },
                      "language": {
                        "type": "string",
                        "description": "Detected or specified language code"
                      },
                      "duration": {
                        "type": "number",
                        "description": "Duration of the audio in seconds"
                      },
                      "text": {
                        "type": "string",
                        "description": "Complete transcribed text"
                      },
                      "segments": {
                        "type": "array",
                        "description": "Array of transcription segments",
                        "items": {
                          "type": "object",
                          "properties": {
                            "id": {
                              "type": "integer",
                              "description": "Segment identifier"
                            },
                            "text": {
                              "type": "string",
                              "description": "Transcribed text for this segment"
                            },
                            "start": {
                              "type": "number",
                              "description": "Start time of the segment in seconds"
                            },
                            "end": {
                              "type": "number",
                              "description": "End time of the segment in seconds"
                            },
                            "language": {
                              "type": "string",
                              "description": "Language code for this segment"
                            },
                            "speaker": {
                              "type": "string",
                              "description": "Speaker identifier when speaker_labels is enabled"
                            },
                            "words": {
                              "type": "array",
                              "description": "Word-level details when timestamp_granularities includes 'word'",
                              "items": {
                                "type": "object",
                                "properties": {
                                  "word": {
                                    "type": "string",
                                    "description": "Individual word"
                                  },
                                  "start": {
                                    "type": "number",
                                    "description": "Start time of the word in seconds"
                                  },
                                  "end": {
                                    "type": "number",
                                    "description": "End time of the word in seconds"
                                  },
                                  "speaker": {
                                    "type": "string",
                                    "description": "Speaker identifier for this word"
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          },
          "security": [
            {
              "BearerAuth": []
            }
          ]
        }
      }
    },
    "components": {
      "securitySchemes": {
        "BearerAuth": {
          "type": "http",
          "scheme": "bearer"
        }
      }
    }
  }
  

================================================
File: next.config.js
================================================
/** @type {import('next').NextConfig} */
const nextConfig = {
  poweredByHeader: false,
  reactStrictMode: true,
};

module.exports = nextConfig;


================================================
File: package.json
================================================
{
  "name": "speech-to-text",
  "version": "0.1.0",
  "description": "A Next.js application that transcribes audio and video files to text using Lemonfox.ai's API",
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "keywords": [
    "speech-to-text",
    "transcription",
    "next.js",
    "lemonfox",
    "audio",
    "video"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/yourusername/speech-to-text"
  },
  "engines": {
    "node": "20.x"
  },
  "packageManager": "yarn@4.1.0",
  "dependencies": {
    "axios": "^1.6.7",
    "next": "14.1.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "autoprefixer": "^10.4.20",
    "eslint": "^8",
    "eslint-config-next": "14.1.0",
    "postcss": "^8",
    "tailwindcss": "^3.4.1",
    "typescript": "^5"
  }
}


================================================
File: postcss.config.js
================================================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};


================================================
File: render.yaml
================================================
version: "1"
services:
  - type: web
    name: speech-to-text
    runtime: node
    buildCommand: yarn install && yarn build
    startCommand: yarn start
    buildFilter:
      paths:
        - "**/*"
      ignoredPaths:
        - ".git/**"
        - "node_modules/**"
    envVars:
      - key: LEMONFOX_API_KEY
        sync: false
      - key: NODE_VERSION
        value: "20.11.1"
    autoDeploy: true
    plan: free
    healthCheckPath: / # Adding health check for the web service 

================================================
File: tailwind.config.ts
================================================
import type { Config } from "tailwindcss";

export default {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
} satisfies Config;


================================================
File: tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}


================================================
File: .dockerignore
================================================
.git
.gitignore
node_modules
npm-debug.log
README.md
.next
.env
.env.*
!.env.example 

================================================
File: .editorconfig
================================================
root = true

[*]
end_of_line = lf
insert_final_newline = true

[*.{js,json,yml}]
charset = utf-8
indent_style = space
indent_size = 2


================================================
File: .yarnrc.yml
================================================
nodeLinker: node-modules

compressionLevel: mixed

enableGlobalCache: false 

================================================
File: src/app/globals.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --foreground-rgb: 0, 0, 0;
    --background-start-rgb: 255, 255, 255;
    --background-end-rgb: 243, 244, 246;
  }
}

@layer components {
  .animate-spin {
    animation: spin 1s linear infinite;
  }
}

@keyframes spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

/* Custom scrollbar for transcription box */
.whitespace-pre-wrap::-webkit-scrollbar {
  width: 8px;
}

.whitespace-pre-wrap::-webkit-scrollbar-track {
  background: #f1f1f1;
  border-radius: 4px;
}

.whitespace-pre-wrap::-webkit-scrollbar-thumb {
  background: #888;
  border-radius: 4px;
}

.whitespace-pre-wrap::-webkit-scrollbar-thumb:hover {
  background: #666;
}


================================================
File: src/app/layout.tsx
================================================
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import "./globals.css";
import AccessGate from "@/components/AccessGate";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Media Transcription Service",
  description:
    "Convert audio and video files to text using Lemonfox.ai's advanced transcription API",
};

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <body className={inter.className}>
        <AccessGate>{children}</AccessGate>
      </body>
    </html>
  );
}


================================================
File: src/app/page.tsx
================================================
import SpeechToText from "@/components/SpeechToText";

export default function Home() {
  return (
    <main className="min-h-screen bg-gradient-to-b from-white to-gray-100 py-12">
      <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div className="text-center mb-12">
          <h1 className="text-4xl font-bold tracking-tight text-gray-900 sm:text-5xl">
            Media Transcription Service
          </h1>
          <p className="mt-4 text-xl text-gray-500">
            Upload your audio/video files or provide a URL to get accurate
            transcriptions powered by Lemonfox.ai
          </p>
        </div>

        <SpeechToText />
      </div>
    </main>
  );
}


================================================
File: src/app/api/transcribe/route.ts
================================================
import { NextRequest, NextResponse } from "next/server";
import FormData from "form-data";
import axios from "axios";
import {
  ALLOWED_FORMATS,
  FILE_LIMITS,
  FILE_SIZE_ERROR,
  AllowedFormat,
} from "@/lib/constants";
import { Readable } from "stream";

const API_URL = "https://api.lemonfox.ai/v1/audio/transcriptions";

// Add runtime config for longer timeout
export const runtime = "nodejs";
export const maxDuration = 3600; // 1 hour in seconds

// Debug logger that only logs in development
const debug = {
  log: (...args: any[]) => {
    if (process.env.NODE_ENV === "development") {
      console.log(...args);
    }
  },
  error: (...args: any[]) => {
    if (process.env.NODE_ENV === "development") {
      console.error(...args);
    }
  },
};

// Helper function to convert File/Blob to Buffer
async function fileToBuffer(file: File): Promise<Buffer> {
  const arrayBuffer = await file.arrayBuffer();
  return Buffer.from(arrayBuffer);
}

export async function POST(request: NextRequest) {
  try {
    debug.log("=== Starting file upload process ===");

    // Check content length for direct file uploads
    const contentLength = request.headers.get("content-length");
    debug.log("Content length:", contentLength);

    if (contentLength && parseInt(contentLength) > FILE_LIMITS.MAX_SIZE) {
      return NextResponse.json(
        { message: FILE_SIZE_ERROR.OVER_LIMIT },
        { status: 413 } // Payload Too Large
      );
    }

    const formData = await request.formData();
    const fileOrUrl = formData.get("file");
    const prompt = formData.get("prompt");
    debug.log("File or URL type:", typeof fileOrUrl);
    debug.log("Is File instance:", fileOrUrl instanceof File);
    if (fileOrUrl instanceof File) {
      debug.log("File details:", {
        name: fileOrUrl.name,
        type: fileOrUrl.type,
        size: fileOrUrl.size,
      });
    }
    debug.log("Prompt received:", prompt);

    if (!fileOrUrl) {
      return NextResponse.json(
        { message: "No file or URL provided" },
        { status: 400 }
      );
    }

    // Create a new server-side FormData instance
    const lemonfoxFormData = new FormData();
    debug.log("Created new FormData for Lemonfox");

    if (typeof fileOrUrl === "string") {
      debug.log("Processing URL upload");
      try {
        new URL(fileOrUrl); // Validate URL format
        lemonfoxFormData.append("file", fileOrUrl);
        debug.log("URL appended to FormData");
      } catch {
        return NextResponse.json(
          { message: "Invalid URL provided" },
          { status: 400 }
        );
      }
    } else if (fileOrUrl instanceof File) {
      debug.log("Processing File upload");
      // Validate file format
      if (!ALLOWED_FORMATS.includes(fileOrUrl.type as AllowedFormat)) {
        debug.log("Invalid file format:", fileOrUrl.type);
        return NextResponse.json(
          { message: FILE_SIZE_ERROR.INVALID_FORMAT(fileOrUrl.type) },
          { status: 400 }
        );
      }

      // Validate file size
      if (fileOrUrl.size > FILE_LIMITS.MAX_SIZE) {
        debug.log("File too large:", fileOrUrl.size);
        return NextResponse.json(
          { message: FILE_SIZE_ERROR.OVER_LIMIT },
          { status: 413 }
        );
      }

      try {
        debug.log("Converting file to buffer");
        const buffer = await fileToBuffer(fileOrUrl);
        debug.log("Buffer created, size:", buffer.length);

        debug.log("Creating readable stream from buffer");
        const stream = Readable.from(buffer);

        debug.log("Appending file to FormData with metadata");
        lemonfoxFormData.append("file", stream, {
          filename: fileOrUrl.name,
          contentType: fileOrUrl.type,
          knownLength: buffer.length,
        });
        debug.log("File successfully appended to FormData");
      } catch (error) {
        debug.error("Error processing file:", error);
        throw error;
      }
    } else {
      debug.log("Invalid input type received:", typeof fileOrUrl);
      return NextResponse.json(
        { message: "Invalid file input" },
        { status: 400 }
      );
    }

    // Add Lemonfox-specific parameters
    debug.log("Adding Lemonfox-specific parameters");
    lemonfoxFormData.append("response_format", "verbose_json");
    lemonfoxFormData.append("speaker_labels", "true");
    if (prompt) {
      lemonfoxFormData.append("prompt", prompt);
      debug.log("Added prompt to FormData");
    }

    // Add callback URL only in production environment
    if (process.env.NODE_ENV === "production") {
      const protocol = "https"; // Always use HTTPS in production
      const host = request.headers.get("host") || "";

      // Skip callback for localhost and invalid hosts
      if (!host || host.includes("localhost") || host.includes("127.0.0.1")) {
        debug.log("Skipping callback URL for localhost/invalid host:", host);
      } else {
        const callbackUrl = `${protocol}://${host}/api/transcription-callback`;
        try {
          // Validate the callback URL
          new URL(callbackUrl);
          lemonfoxFormData.append("callback_url", callbackUrl);
          debug.log("Added callback URL:", callbackUrl);
        } catch (error) {
          debug.error("Invalid callback URL format:", callbackUrl);
        }
      }
    } else {
      debug.log("Skipping callback URL in development environment");
    }

    // Validate API key exists
    const apiKey = process.env.LEMONFOX_API_KEY;
    if (!apiKey) {
      debug.error("LEMONFOX_API_KEY is not configured");
      return NextResponse.json(
        { message: "Service configuration error" },
        { status: 500 }
      );
    }

    debug.log("Preparing to make API call to Lemonfox");
    // Call Lemonfox API with the form-data
    const response = await axios.post(API_URL, lemonfoxFormData, {
      headers: {
        Authorization: `Bearer ${apiKey}`,
        ...lemonfoxFormData.getHeaders(), // Important for multipart/form-data
      },
      maxBodyLength: FILE_LIMITS.URL_MAX_SIZE,
      timeout: 3600000, // 1 hour timeout
      timeoutErrorMessage:
        "Transcription request timed out. Please try again with a shorter audio file or use the URL method for large files.",
    });
    debug.log("API call successful, processing response");

    // Extract text and segments from verbose_json response
    const { text, segments } = response.data;
    debug.log("Extracted text and segments from response");

    // Structure the conversation based on segments
    const structuredConversation = segments.map((segment: any) => ({
      role: segment.speaker,
      text: segment.text,
      timestamp: {
        start: segment.start,
        end: segment.end,
      },
    }));
    debug.log("Structured conversation created");

    return NextResponse.json({ text, segments, structuredConversation });
  } catch (error) {
    debug.error("Transcription error:", error);

    if (axios.isAxiosError(error)) {
      // Log the full error response for debugging
      debug.error("API Error Response:", error.response?.data);
      debug.error("API Error Status:", error.response?.status);
      debug.error("API Error Headers:", error.response?.headers);

      const statusCode = error.response?.status || 500;
      let message = "Failed to transcribe audio/video";

      // Use the API's error message if available
      if (error.response?.data?.error?.message) {
        message = error.response.data.error.message;
      }

      // Enhanced error status code logging and handling
      switch (statusCode) {
        case 400:
          debug.error("Bad Request: Invalid parameters or malformed request");
          message = "Invalid request parameters";
          break;
        case 401:
        case 403:
          debug.error("Authentication Error: Invalid or missing API key");
          message = "Service authentication error";
          break;
        case 413:
          debug.error("File Size Error: File exceeds maximum size limit");
          message = FILE_SIZE_ERROR.OVER_LIMIT;
          break;
        case 415:
          debug.error("Unsupported Media Type: Invalid file format");
          message = FILE_SIZE_ERROR.INVALID_FORMAT("unknown format");
          break;
        case 429:
          debug.error("Rate Limit Error: Too many requests");
          message = "Rate limit exceeded. Please try again later";
          break;
        default:
          debug.error(`Unexpected Error Status Code: ${statusCode}`);
      }

      return NextResponse.json(
        {
          message,
          details:
            error.response?.data?.error || "No additional details available",
          status: statusCode,
        },
        { status: statusCode }
      );
    }

    debug.error("Non-Axios error occurred:", error);
    return NextResponse.json(
      {
        message: "Internal server error",
        status: 500,
        details: error instanceof Error ? error.message : "Unknown error",
      },
      { status: 500 }
    );
  }
}


================================================
File: src/app/api/transcription-callback/route.ts
================================================
import { NextRequest, NextResponse } from "next/server";
import { TranscriptionResult } from "@/components/SpeechToText/types";
import { transcriptionStore } from "@/lib/transcriptionStore";

// Validate that the request is coming from Lemonfox
const validateRequest = (request: NextRequest): boolean => {
  const authHeader = request.headers.get("Authorization");
  const apiKey = process.env.LEMONFOX_API_KEY;

  if (!apiKey) {
    console.error("LEMONFOX_API_KEY is not configured");
    return false;
  }

  return authHeader === `Bearer ${apiKey}`;
};

export async function POST(request: NextRequest) {
  try {
    // Validate the request
    if (!validateRequest(request)) {
      console.warn(
        "Unauthorized callback attempt with invalid or missing API key"
      );
      return NextResponse.json({ message: "Unauthorized" }, { status: 401 });
    }

    // Parse and validate the transcription result
    const transcriptionResult: TranscriptionResult = await request.json();

    // Basic validation of required fields
    if (
      !transcriptionResult.text ||
      !Array.isArray(transcriptionResult.segments)
    ) {
      console.error(
        "Invalid transcription result format:",
        transcriptionResult
      );
      return NextResponse.json(
        { message: "Invalid transcription result format" },
        { status: 400 }
      );
    }

    // Store the result and get an ID
    const resultId = transcriptionStore.store(transcriptionResult);

    // Development logging
    if (process.env.NODE_ENV === "development") {
      console.log("Received transcription callback for resultId:", resultId);
      console.log("Text length:", transcriptionResult.text.length);
      console.log("Number of segments:", transcriptionResult.segments.length);
    }

    return NextResponse.json({ success: true, resultId });
  } catch (error) {
    console.error("Error handling transcription callback:", error);
    return NextResponse.json(
      { message: "Internal server error", error: String(error) },
      { status: 500 }
    );
  }
}


================================================
File: src/app/api/transcription-status/[resultId]/route.ts
================================================
import { NextRequest, NextResponse } from "next/server";
import { transcriptionStore } from "@/lib/transcriptionStore";

export async function GET(
  request: NextRequest,
  { params }: { params: { resultId: string } }
) {
  const { resultId } = params;

  // Check if we have results for this ID
  const result = transcriptionStore.get(resultId);

  if (!result) {
    return NextResponse.json(
      { message: "Transcription not found" },
      { status: 404 }
    );
  }

  return NextResponse.json(result);
}


================================================
File: src/components/LanguageSelector.tsx
================================================
"use client";

import { useState } from "react";

// Complete list of supported languages from Lemonfox API
const languages = [
  { code: "indonesian", name: "Indonesian" },
  { code: "english", name: "English" },
  { code: "chinese", name: "Chinese" },
  { code: "german", name: "German" },
  { code: "spanish", name: "Spanish" },
  { code: "russian", name: "Russian" },
  { code: "korean", name: "Korean" },
  { code: "french", name: "French" },
  { code: "japanese", name: "Japanese" },
  { code: "portuguese", name: "Portuguese" },
  { code: "turkish", name: "Turkish" },
  { code: "polish", name: "Polish" },
  { code: "catalan", name: "Catalan" },
  { code: "dutch", name: "Dutch" },
  { code: "arabic", name: "Arabic" },
  { code: "swedish", name: "Swedish" },
  { code: "italian", name: "Italian" },
  { code: "hindi", name: "Hindi" },
  { code: "finnish", name: "Finnish" },
  { code: "vietnamese", name: "Vietnamese" },
  { code: "hebrew", name: "Hebrew" },
  { code: "ukrainian", name: "Ukrainian" },
  { code: "greek", name: "Greek" },
  { code: "malay", name: "Malay" },
  { code: "czech", name: "Czech" },
  { code: "romanian", name: "Romanian" },
  { code: "danish", name: "Danish" },
  { code: "hungarian", name: "Hungarian" },
  { code: "tamil", name: "Tamil" },
  { code: "norwegian", name: "Norwegian" },
  { code: "thai", name: "Thai" },
  { code: "urdu", name: "Urdu" },
  { code: "croatian", name: "Croatian" },
  { code: "bulgarian", name: "Bulgarian" },
  { code: "lithuanian", name: "Lithuanian" },
  { code: "latin", name: "Latin" },
  { code: "maori", name: "Maori" },
  { code: "malayalam", name: "Malayalam" },
  { code: "welsh", name: "Welsh" },
  { code: "slovak", name: "Slovak" },
  { code: "telugu", name: "Telugu" },
  { code: "persian", name: "Persian" },
  { code: "latvian", name: "Latvian" },
  { code: "bengali", name: "Bengali" },
  { code: "serbian", name: "Serbian" },
  { code: "azerbaijani", name: "Azerbaijani" },
  { code: "slovenian", name: "Slovenian" },
  { code: "kannada", name: "Kannada" },
  { code: "estonian", name: "Estonian" },
  { code: "macedonian", name: "Macedonian" },
  { code: "breton", name: "Breton" },
  { code: "basque", name: "Basque" },
  { code: "icelandic", name: "Icelandic" },
  { code: "armenian", name: "Armenian" },
  { code: "nepali", name: "Nepali" },
  { code: "mongolian", name: "Mongolian" },
  { code: "bosnian", name: "Bosnian" },
  { code: "kazakh", name: "Kazakh" },
  { code: "albanian", name: "Albanian" },
  { code: "swahili", name: "Swahili" },
  { code: "galician", name: "Galician" },
  { code: "marathi", name: "Marathi" },
  { code: "punjabi", name: "Punjabi" },
  { code: "sinhala", name: "Sinhala" },
  { code: "khmer", name: "Khmer" },
  { code: "shona", name: "Shona" },
  { code: "yoruba", name: "Yoruba" },
  { code: "somali", name: "Somali" },
  { code: "afrikaans", name: "Afrikaans" },
  { code: "occitan", name: "Occitan" },
  { code: "georgian", name: "Georgian" },
  { code: "belarusian", name: "Belarusian" },
  { code: "tajik", name: "Tajik" },
  { code: "sindhi", name: "Sindhi" },
  { code: "gujarati", name: "Gujarati" },
  { code: "amharic", name: "Amharic" },
  { code: "yiddish", name: "Yiddish" },
  { code: "lao", name: "Lao" },
  { code: "uzbek", name: "Uzbek" },
  { code: "faroese", name: "Faroese" },
  { code: "haitian creole", name: "Haitian Creole" },
  { code: "pashto", name: "Pashto" },
  { code: "turkmen", name: "Turkmen" },
  { code: "nynorsk", name: "Nynorsk" },
  { code: "maltese", name: "Maltese" },
  { code: "sanskrit", name: "Sanskrit" },
  { code: "luxembourgish", name: "Luxembourgish" },
  { code: "myanmar", name: "Myanmar" },
  { code: "tibetan", name: "Tibetan" },
  { code: "tagalog", name: "Tagalog" },
  { code: "malagasy", name: "Malagasy" },
  { code: "assamese", name: "Assamese" },
  { code: "tatar", name: "Tatar" },
  { code: "hawaiian", name: "Hawaiian" },
  { code: "lingala", name: "Lingala" },
  { code: "hausa", name: "Hausa" },
  { code: "bashkir", name: "Bashkir" },
  { code: "javanese", name: "Javanese" },
  { code: "sundanese", name: "Sundanese" },
  { code: "cantonese", name: "Cantonese" },
  { code: "burmese", name: "Burmese" },
  { code: "valencian", name: "Valencian" },
  { code: "flemish", name: "Flemish" },
  { code: "haitian", name: "Haitian" },
  { code: "letzeburgesch", name: "Letzeburgesch" },
  { code: "pushto", name: "Pushto" },
  { code: "panjabi", name: "Panjabi" },
  { code: "moldavian", name: "Moldavian" },
  { code: "moldovan", name: "Moldovan" },
  { code: "sinhalese", name: "Sinhalese" },
  { code: "castilian", name: "Castilian" },
  { code: "mandarin", name: "Mandarin" },
].sort((a, b) => a.name.localeCompare(b.name)); // Sort alphabetically by name

interface LanguageSelectorProps {
  onLanguageChange: (language: string) => void;
}

export default function LanguageSelector({
  onLanguageChange,
}: LanguageSelectorProps) {
  const [selectedLanguage, setSelectedLanguage] = useState("indonesian");

  const handleLanguageChange = (e: React.ChangeEvent<HTMLSelectElement>) => {
    const newLanguage = e.target.value;
    setSelectedLanguage(newLanguage);
    onLanguageChange(newLanguage);
  };

  return (
    <div className="flex flex-col gap-2">
      <label
        htmlFor="language-select"
        className="text-sm font-medium text-gray-700"
      >
        Select Language
      </label>
      <select
        id="language-select"
        value={selectedLanguage}
        onChange={handleLanguageChange}
        className="block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm"
      >
        {languages.map((language) => (
          <option key={language.code} value={language.code}>
            {language.name}
          </option>
        ))}
      </select>
    </div>
  );
}


================================================
File: src/components/AccessForm/index.tsx
================================================
"use client";

import { useState } from "react";
import { validateAccessCode, setAuthenticated } from "@/lib/authUtils";

export default function AccessForm() {
  const [code, setCode] = useState("");
  const [error, setError] = useState("");

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();

    if (validateAccessCode(code)) {
      setAuthenticated(true);
      window.location.reload(); // Reload to update auth state
    } else {
      setError("Invalid access code. Please try again.");
      setCode("");
    }
  };

  return (
    <div className="min-h-screen flex items-center justify-center bg-gray-100">
      <div className="max-w-md w-full space-y-8 p-8 bg-white rounded-lg shadow-md">
        <div>
          <h2 className="mt-6 text-center text-3xl font-extrabold text-gray-900">
            Protected Content
          </h2>
          <p className="mt-2 text-center text-sm text-gray-600">
            Please enter the access code to continue
          </p>
        </div>
        <form className="mt-8 space-y-6" onSubmit={handleSubmit}>
          <div>
            <label htmlFor="access-code" className="sr-only">
              Access Code
            </label>
            <input
              id="access-code"
              name="code"
              type="password"
              required
              value={code}
              onChange={(e) => setCode(e.target.value)}
              className="appearance-none rounded-md relative block w-full px-3 py-2 border border-gray-300 placeholder-gray-500 text-gray-900 focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 focus:z-10 sm:text-sm"
              placeholder="Enter access code"
            />
          </div>
          {error && <p className="text-red-500 text-sm text-center">{error}</p>}
          <div>
            <button
              type="submit"
              className="group relative w-full flex justify-center py-2 px-4 border border-transparent text-sm font-medium rounded-md text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500"
            >
              Submit
            </button>
          </div>
        </form>
      </div>
    </div>
  );
}


================================================
File: src/components/AccessGate/index.tsx
================================================
"use client";

import { useEffect, useState } from "react";
import { isAuthenticated } from "@/lib/authUtils";
import AccessForm from "@/components/AccessForm";

interface AccessGateProps {
  children: React.ReactNode;
}

export default function AccessGate({ children }: AccessGateProps) {
  const [authorized, setAuthorized] = useState(false);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    setAuthorized(isAuthenticated());
    setLoading(false);
  }, []);

  if (loading) {
    return (
      <div className="min-h-screen flex items-center justify-center bg-gray-100">
        <div className="text-gray-600">Loading...</div>
      </div>
    );
  }

  if (!authorized) {
    return <AccessForm />;
  }

  return <>{children}</>;
}


================================================
File: src/components/SpeechToText/FileDropZone.tsx
================================================
import { useCallback, useMemo, useRef, useState } from "react";
import { ALLOWED_FORMATS, FILE_LIMITS, FILE_SIZE_ERROR } from "@/lib/constants";

interface FileDropZoneProps {
  onFileSelect: (file: File) => void;
  onError: (message: string) => void;
  isTranscribing: boolean;
  currentFile: File | null;
}

interface FormatGroups {
  audio: string[];
  video: string[];
}

function isAllowedMimeType(
  mimeType: string
): mimeType is (typeof ALLOWED_FORMATS)[number] {
  return ALLOWED_FORMATS.includes(mimeType as (typeof ALLOWED_FORMATS)[number]);
}

export function FileDropZone({
  onFileSelect,
  onError,
  isTranscribing,
  currentFile,
}: FileDropZoneProps) {
  const [isDragging, setIsDragging] = useState(false);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const validateFile = useCallback(
    (file: File): boolean => {
      if (!isAllowedMimeType(file.type)) {
        onError(FILE_SIZE_ERROR.INVALID_FORMAT(file.type || "unknown"));
        return false;
      }

      if (file.size > FILE_LIMITS.MAX_SIZE) {
        onError(FILE_SIZE_ERROR.OVER_LIMIT);
        return false;
      }

      return true;
    },
    [onError]
  );

  const handleDrop = useCallback(
    (e: React.DragEvent<HTMLDivElement>) => {
      e.preventDefault();
      setIsDragging(false);

      const file = e.dataTransfer.files[0];
      if (file && validateFile(file)) {
        onFileSelect(file);
      }
    },
    [onFileSelect, validateFile]
  );

  const handleDragOver = useCallback((e: React.DragEvent<HTMLDivElement>) => {
    e.preventDefault();
    setIsDragging(true);
  }, []);

  const handleDragLeave = useCallback((e: React.DragEvent<HTMLDivElement>) => {
    e.preventDefault();
    setIsDragging(false);
  }, []);

  const handleFileInput = useCallback(
    (e: React.ChangeEvent<HTMLInputElement>) => {
      const file = e.target.files?.[0];
      if (file && validateFile(file)) {
        onFileSelect(file);
      }
    },
    [onFileSelect, validateFile]
  );

  const handleClick = () => {
    fileInputRef.current?.click();
  };

  const formatGroups: FormatGroups = useMemo(
    () => ({
      audio: ALLOWED_FORMATS.filter((f) => f.startsWith("audio/")).map((f) =>
        f.replace("audio/", "")
      ),
      video: ALLOWED_FORMATS.filter((f) => f.startsWith("video/")).map((f) =>
        f.replace("video/", "")
      ),
    }),
    []
  );

  const getObjectURL = useCallback(
    (file: File) => URL.createObjectURL(file),
    []
  );

  return (
    <div className="space-y-4">
      <div
        role="button"
        tabIndex={0}
        aria-label={
          currentFile
            ? "Selected media file preview"
            : "Drop zone for audio or video files"
        }
        className={`border-2 border-dashed rounded-lg p-8 text-center cursor-pointer transition-colors
          ${
            isDragging
              ? "border-emerald-500 bg-emerald-50"
              : "border-gray-300 hover:border-gray-400"
          }
          ${isTranscribing ? "pointer-events-none opacity-50" : ""}
          focus:outline-none focus:ring-2 focus:ring-emerald-500 focus:ring-offset-2`}
        onDrop={handleDrop}
        onDragOver={handleDragOver}
        onDragLeave={handleDragLeave}
        onClick={handleClick}
        onKeyDown={(e) => {
          if (e.key === "Enter" || e.key === " ") {
            e.preventDefault();
            handleClick();
          }
        }}
        aria-busy={isTranscribing}
      >
        <input
          type="file"
          ref={fileInputRef}
          className="hidden"
          accept={ALLOWED_FORMATS.join(",")}
          onChange={handleFileInput}
          aria-hidden="true"
        />

        {isTranscribing ? (
          <div className="flex flex-col items-center space-y-2" role="status">
            <div
              className="animate-spin rounded-full h-8 w-8 border-b-2 border-emerald-500"
              aria-hidden="true"
            ></div>
            <p className="text-gray-600">
              <span aria-hidden="true">🎵</span>
              Transcribing media...
            </p>
            <div className="sr-only">
              Processing media file for transcription
            </div>
          </div>
        ) : (
          <div className="space-y-4">
            <div className="text-gray-600">
              {currentFile ? (
                <>
                  <p className="font-medium" id="selected-file">
                    {currentFile.name}
                  </p>
                  {currentFile.type.startsWith("audio/") ? (
                    <audio
                      controls
                      src={getObjectURL(currentFile)}
                      className="mt-4"
                      aria-label={`Audio preview for ${currentFile.name}`}
                    />
                  ) : (
                    <video
                      controls
                      src={getObjectURL(currentFile)}
                      className="mt-4 max-w-full"
                      aria-label={`Video preview for ${currentFile.name}`}
                    />
                  )}
                </>
              ) : (
                <>
                  <p
                    className="font-medium text-emerald-700"
                    id="dropzone-instructions"
                  >
                    <span aria-hidden="true">📁</span> Drag and drop a file
                    here, or click to select
                  </p>
                  <div
                    className="text-sm mt-2 space-y-1"
                    role="region"
                    aria-label="File format requirements"
                  >
                    <p>
                      <span aria-hidden="true">🎵</span> Supported audio
                      formats:{" "}
                      <span
                        aria-label={`Supported audio formats: ${formatGroups.audio.join(
                          ", "
                        )}`}
                      >
                        {formatGroups.audio.join(", ").toUpperCase()}
                      </span>
                    </p>
                    <p>
                      <span aria-hidden="true">🎥</span> Supported video
                      formats:{" "}
                      <span
                        aria-label={`Supported video formats: ${formatGroups.video.join(
                          ", "
                        )}`}
                      >
                        {formatGroups.video.join(", ").toUpperCase()}
                      </span>
                    </p>
                    <p>
                      <span aria-hidden="true">📁</span> Size limits: Direct
                      upload up to {FILE_LIMITS.MAX_SIZE / 1024 / 1024}
                      MB. For larger files up to{" "}
                      {FILE_LIMITS.URL_MAX_SIZE / 1024 / 1024}MB, please use URL
                      upload.
                    </p>
                  </div>
                </>
              )}
            </div>
          </div>
        )}
      </div>
    </div>
  );
}


================================================
File: src/components/SpeechToText/TranscriptionResult.tsx
================================================
import { useState } from "react";
import { Toast } from "../Toast";
import { Segment, StructuredConversation } from "./types";

type TranscriptionResultProps = {
  text: string;
  segments?: Segment[];
  structuredConversation?: StructuredConversation[];
  error?: string;
  onCopy: (format: "raw" | "verbose") => Promise<void>;
};

export function TranscriptionResult({
  text,
  segments,
  structuredConversation,
  error,
  onCopy,
}: TranscriptionResultProps) {
  const [showToast, setShowToast] = useState(false);
  const [copyFormat, setCopyFormat] = useState<"raw" | "verbose">("raw");

  const handleCopy = async () => {
    try {
      await onCopy(copyFormat);
      setShowToast(true);
    } catch (error) {
      console.error("Failed to copy text:", error);
    }
  };

  return (
    <div className="mt-8">
      {error ? (
        <div className="text-rose-500">❌ {error}</div>
      ) : (
        <div className="space-y-4">
          <div className="flex justify-between items-center">
            <h3
              className="text-lg font-medium text-emerald-700"
              id="transcription-heading"
            >
              <span aria-hidden="true">📝</span> Transcription
            </h3>
            <div className="flex items-center gap-2">
              <select
                value={copyFormat}
                onChange={(e) =>
                  setCopyFormat(e.target.value as "raw" | "verbose")
                }
                className="text-sm border border-emerald-200 rounded px-2 py-1 focus:outline-none focus:ring-2 focus:ring-emerald-500"
                aria-label="Select copy format"
              >
                <option value="raw">Raw Text</option>
                <option value="verbose">Detailed (with Speakers)</option>
              </select>
              <button
                onClick={handleCopy}
                className="px-4 py-2 text-sm bg-emerald-500 text-white rounded hover:bg-emerald-600 transition-colors flex items-center gap-2 focus:outline-none focus:ring-2 focus:ring-emerald-500 focus:ring-offset-2"
                aria-label="Copy transcription to clipboard"
              >
                <span aria-hidden="true">📋</span> Copy to Clipboard
              </button>
            </div>
          </div>
          <div
            className="p-4 bg-emerald-50 rounded-lg border border-emerald-100"
            aria-labelledby="transcription-heading"
          >
            {structuredConversation ? (
              <div className="space-y-4">
                {structuredConversation.map((item, index) => (
                  <div key={index} className="flex gap-4 items-start">
                    <div className="flex-shrink-0 w-24">
                      <span className="inline-block px-2 py-1 bg-emerald-100 rounded text-sm font-medium text-emerald-800">
                        {item.role}
                      </span>
                    </div>
                    <div className="flex-grow">
                      <p className="text-gray-800">{item.text}</p>
                      {item.timestamp && (
                        <p className="text-xs text-gray-500 mt-1">
                          {Math.floor(item.timestamp.start)}s -{" "}
                          {Math.floor(item.timestamp.end)}s
                        </p>
                      )}
                    </div>
                  </div>
                ))}
              </div>
            ) : segments ? (
              <div className="space-y-4">
                {segments.map((segment, index) => (
                  <div key={index} className="flex gap-4 items-start">
                    <div className="flex-shrink-0 w-24">
                      <span className="inline-block px-2 py-1 bg-emerald-100 rounded text-sm font-medium text-emerald-800">
                        {segment.speaker}
                      </span>
                    </div>
                    <div className="flex-grow">
                      <p className="text-gray-800">{segment.text}</p>
                      <p className="text-xs text-gray-500 mt-1">
                        {Math.floor(segment.start)}s - {Math.floor(segment.end)}
                        s
                      </p>
                    </div>
                  </div>
                ))}
              </div>
            ) : (
              <div className="whitespace-pre-wrap">{text}</div>
            )}
          </div>
        </div>
      )}
      {showToast && (
        <Toast
          message="Transcription copied to clipboard!"
          onClose={() => setShowToast(false)}
        />
      )}
    </div>
  );
}


================================================
File: src/components/SpeechToText/URLInput.tsx
================================================
import { useState } from "react";

interface URLInputProps {
  onSubmit: (url: string) => Promise<void>;
  isTranscribing: boolean;
}

export function URLInput({ onSubmit, isTranscribing }: URLInputProps) {
  const [urlInput, setUrlInput] = useState("");
  const inputId = "url-input";
  const buttonId = "submit-url";

  const handleSubmit = () => {
    onSubmit(urlInput);
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === "Enter") {
      handleSubmit();
    }
  };

  return (
    <div className="flex gap-2" role="group" aria-labelledby={buttonId}>
      <input
        id={inputId}
        type="text"
        placeholder="Enter media URL (optional)"
        value={urlInput}
        onChange={(e) => setUrlInput(e.target.value)}
        onKeyDown={handleKeyDown}
        className="flex-1 px-4 py-2 border rounded-lg focus:ring-2 focus:ring-emerald-500 focus:border-emerald-500 focus:outline-none"
        aria-label="Media URL input"
      />
      <button
        id={buttonId}
        onClick={handleSubmit}
        disabled={isTranscribing}
        className="px-4 py-2 bg-emerald-500 text-white rounded-lg hover:bg-emerald-600 disabled:opacity-50 focus:outline-none focus:ring-2 focus:ring-emerald-500 focus:ring-offset-2"
        aria-busy={isTranscribing}
      >
        {isTranscribing ? "Transcribing..." : "Transcribe URL"}
      </button>
    </div>
  );
}


================================================
File: src/components/SpeechToText/index.tsx
================================================
"use client";

import { SpeechToTextProps, TRANSCRIPTION_STATUS_MESSAGES } from "./types";
import { useSpeechToText } from "@/hooks/useSpeechToText";
import { URLInput } from "./URLInput";
import { FileDropZone } from "./FileDropZone";
import { TranscriptionResult } from "./TranscriptionResult";
import LanguageSelector from "../LanguageSelector";
import { useState } from "react";

export default function SpeechToText({
  onTranscriptionComplete,
  onError,
}: SpeechToTextProps) {
  const [selectedLanguage, setSelectedLanguage] = useState("id");
  const {
    input,
    status: transcriptionStatus,
    transcription,
    handleFile,
    handleUrl,
    handleError,
    getProcessingTime,
  } = useSpeechToText({
    onTranscriptionComplete,
    onError,
    language: selectedLanguage,
  });

  return (
    <div className="w-full max-w-2xl mx-auto p-6 space-y-6">
      <div className="space-y-6">
        <LanguageSelector onLanguageChange={setSelectedLanguage} />

        <URLInput
          onSubmit={handleUrl}
          isTranscribing={
            transcriptionStatus === "uploading" ||
            transcriptionStatus === "processing"
          }
        />

        <FileDropZone
          onFileSelect={handleFile}
          onError={handleError}
          isTranscribing={
            transcriptionStatus === "uploading" ||
            transcriptionStatus === "processing"
          }
          currentFile={input?.type === "file" ? (input.data as File) : null}
        />
      </div>

      {transcriptionStatus !== "idle" && (
        <div
          className="mt-4 text-center text-gray-600"
          role="status"
          aria-live="polite"
        >
          <p>
            {TRANSCRIPTION_STATUS_MESSAGES[transcriptionStatus]}
            {transcriptionStatus === "processing" && (
              <span className="text-sm ml-2">({getProcessingTime()})</span>
            )}
          </p>
        </div>
      )}

      {transcription && (
        <TranscriptionResult
          text={transcription.text}
          segments={transcription.segments}
          structuredConversation={transcription.structuredConversation}
          onCopy={async (format) => {
            try {
              await navigator.clipboard.writeText(
                format === "raw"
                  ? transcription.text
                  : JSON.stringify(transcription, null, 2)
              );
            } catch (error) {
              handleError("Failed to copy to clipboard");
            }
          }}
        />
      )}
    </div>
  );
}


================================================
File: src/components/SpeechToText/types.ts
================================================
/**
 * Represents an audio input source that can be either a File object or a URL string.
 * @example
 * // File input from user's local system
 * const fileInput: AudioInput = new File([blob], 'recording.mp3');
 *
 * // URL input from remote source
 * const urlInput: AudioInput = 'https://example.com/audio/recording.mp3';
 */
export type AudioInput = File | string; // string for URL

/**
 * Represents a single word in the transcription with timing and speaker information.
 * @example
 * // Example word object in the transcription
 * const word: Word = {
 *   word: "Hello",
 *   start: 0.5,    // Start time in seconds
 *   end: 0.9,      // End time in seconds
 *   score: 0.98,   // Confidence score (0-1)
 *   speaker: "A"   // Speaker identifier
 * };
 */
export interface Word {
  word: string;
  start: number;
  end: number;
  score: number;
  speaker: string;
}

/**
 * Represents a segment of transcribed speech with metadata.
 * Each segment contains multiple words and is associated with a specific speaker.
 *
 * @example
 * // Example segment in the transcription
 * const segment: Segment = {
 *   id: 1,
 *   text: "Hello, how are you today?",
 *   start: 0.5,
 *   end: 2.3,
 *   avg_logprob: -0.12,      // Average log probability (confidence)
 *   language: "en",          // Detected language
 *   speaker: "A",            // Speaker identifier
 *   words: [                 // Detailed word-level information
 *         {
 *           word: "Hello",
 *           start: 0.5,
 *           end: 0.9,
 *           score: 0.98,
 *           speaker: "A"
 *         },
 *         {
 *           word: "how",
 *           start: 1.0,
 *           end: 1.2,
 *           score: 0.95,
 *           speaker: "A"
 *         },
 *         {
 *           word: "are",
 *           start: 1.3,
 *           end: 1.5,
 *           score: 0.97,
 *           speaker: "A"
 *         },
 *         {
 *           word: "you",
 *           start: 1.6,
 *           end: 1.8,
 *           score: 0.96,
 *           speaker: "A"
 *         },
 *         {
 *           word: "today",
 *           start: 1.9,
 *           end: 2.3,
 *           score: 0.94,
 *           speaker: "A"
 *         }
 *       ]
 *     },
 *     // ... more words
 *   ]
 * };
 *
 * UI Visualization:
 * +----------------------------------+
 * | Speaker A                    0:00 |
 * | Hello, how are you today?         |
 * |                                   |
 * | Speaker B                    0:02 |
 * | I'm doing great, thank you!       |
 * +----------------------------------+
 */
export interface Segment {
  id: number;
  text: string;
  start: number;
  end: number;
  avg_logprob: number;
  language: string;
  speaker: string;
  words: Word[];
}

/**
 * Represents a structured conversation format for the transcription
 */
export interface StructuredConversation {
  role: string;
  text: string;
  timestamp?: {
    start: number;
    end: number;
  };
}

/**
 * Represents the complete transcription result with all segments and potential error information.
 *
 * @example
 * // Example transcription result
 * const result: TranscriptionResult = {
 *   text: "Hello, how are you today? I'm doing great, thank you!",
 *   segments: [
 *     {
 *       id: 1,
 *       text: "Hello, how are you today?",
 *       start: 0.5,
 *       end: 2.3,
 *       avg_logprob: -0.12,
 *       language: "en",
 *       speaker: "A",
 *       words: [
 *         {
 *           word: "Hello",
 *           start: 0.5,
 *           end: 0.9,
 *           score: 0.98,
 *           speaker: "A"
 *         },
 *         {
 *           word: "how",
 *           start: 1.0,
 *           end: 1.2,
 *           score: 0.95,
 *           speaker: "A"
 *         },
 *         {
 *           word: "are",
 *           start: 1.3,
 *           end: 1.5,
 *           score: 0.97,
 *           speaker: "A"
 *         },
 *         {
 *           word: "you",
 *           start: 1.6,
 *           end: 1.8,
 *           score: 0.96,
 *           speaker: "A"
 *         },
 *         {
 *           word: "today",
 *           start: 1.9,
 *           end: 2.3,
 *           score: 0.94,
 *           speaker: "A"
 *         }
 *       ]
 *     },
 *     {
 *       id: 2,
 *       text: "I'm doing great, thank you!",
 *       start: 2.5,
 *       end: 4.2,
 *       avg_logprob: -0.08,
 *       language: "en",
 *       speaker: "B",
 *       words: [
 *         {
 *           word: "I'm",
 *           start: 2.5,
 *           end: 2.7,
 *           score: 0.96,
 *           speaker: "B"
 *         },
 *         {
 *           word: "doing",
 *           start: 2.8,
 *           end: 3.1,
 *           score: 0.97,
 *           speaker: "B"
 *         },
 *         {
 *           word: "great",
 *           start: 3.2,
 *           end: 3.5,
 *           score: 0.98,
 *           speaker: "B"
 *         },
 *         {
 *           word: "thank",
 *           start: 3.6,
 *           end: 3.8,
 *           score: 0.95,
 *           speaker: "B"
 *         },
 *         {
 *           word: "you",
 *           start: 3.9,
 *           end: 4.2,
 *           score: 0.96,
 *           speaker: "B"
 *         }
 *       ]
 *     }
 *   ]
 * };
 */
export interface TranscriptionResult {
  text: string;
  segments: Segment[];
  error?: string;
  structuredConversation?: StructuredConversation[];
}

/**
 * Represents the current status of a transcription process
 */
export type TranscriptionStatus =
  | "idle"
  | "uploading"
  | "processing"
  | "completed"
  | "error";

/**
 * Props for the SpeechToText component that handles audio transcription.
 *
 * @example
 * // Example usage in a React component
 * <SpeechToText
 *   onTranscriptionComplete={(result) => {
 *     console.log('Full transcription:', result.text);
 *     result.segments.forEach(segment => {
 *       console.log(`Speaker ${segment.speaker}: ${segment.text}`);
 *     });
 *   }}
 *   onError={(error) => {
 *     console.error('Transcription error:', error);
 *   }}
 * />
 */
export interface SpeechToTextProps {
  onTranscriptionComplete?: (result: TranscriptionResult) => void;
  onError?: (error: string) => void;
}

/**
 * Status messages for different transcription stages
 */
export const TRANSCRIPTION_STATUS_MESSAGES = {
  idle: "",
  uploading: "Uploading your media file...",
  processing:
    "Processing your transcription. This may take several minutes for longer files...",
  completed: "Transcription completed!",
  error: "An error occurred during transcription.",
} as const;


================================================
File: src/components/Toast/index.tsx
================================================
import { useEffect } from "react";

interface ToastProps {
  message: string;
  type?: "success" | "error";
  duration?: number;
  onClose: () => void;
}

export function Toast({
  message,
  type = "success",
  duration = 3000,
  onClose,
}: ToastProps) {
  useEffect(() => {
    const timer = setTimeout(onClose, duration);
    return () => clearTimeout(timer);
  }, [duration, onClose]);

  return (
    <div
      role="alert"
      aria-live="polite"
      className={`fixed bottom-4 right-4 px-6 py-3 rounded-lg shadow-lg text-white
        ${type === "success" ? "bg-emerald-500" : "bg-rose-500"}
        transition-opacity duration-300 flex items-center gap-2`}
    >
      <span aria-hidden="true">{type === "success" ? "✅" : "❌"}</span>
      {message}
    </div>
  );
}


================================================
File: src/hooks/useSpeechToText.ts
================================================
import { useState, useCallback, useEffect } from "react";
import {
  TranscriptionResult,
  TranscriptionStatus,
} from "@/components/SpeechToText/types";
import { transcribeAudio } from "@/lib/transcriptionService";

// Import polling configuration from transcriptionService
const POLLING_INTERVAL = 2000; // Match initial interval from transcriptionService
const MAX_POLLING_INTERVAL = 10000; // Maximum polling interval

interface TranscriptionResponse extends TranscriptionResult {
  resultId?: string;
}

interface UseSpeechToTextInput {
  type: "file" | "url";
  data: File | string;
}

interface UseSpeechToTextState {
  /** Current input file or URL being processed */
  input: UseSpeechToTextInput | null;
  /** Current status of the transcription process */
  status: TranscriptionStatus;
  /** The transcription result or error */
  transcription: TranscriptionResult | null;
  /** Start time of the processing, used to show duration */
  processingStartTime: Date | null;
  /** ID for polling transcription status */
  transcriptionId: string | null;
}

interface UseSpeechToTextCallbacks {
  /** Callback when transcription is successfully completed */
  onTranscriptionComplete?: (result: TranscriptionResult) => void;
  /** Callback when an error occurs during transcription */
  onError?: (error: string) => void;
  /** Selected language for transcription */
  language?: string;
}

interface UseSpeechToTextReturn
  extends Omit<UseSpeechToTextState, "processingStartTime"> {
  /** Handle file input for transcription */
  handleFile: (file: File) => Promise<void>;
  /** Handle URL input for transcription */
  handleUrl: (url: string) => Promise<void>;
  /** Handle general errors */
  handleError: (error: string) => void;
  /** Get the current processing time in a human-readable format */
  getProcessingTime: () => string;
  /** Whether there's an active transcription that should prevent tab closing */
  isActiveTranscription: boolean;
  transcriptionId: string | null;
}

/**
 * A custom hook that manages the speech-to-text transcription process.
 *
 * This hook handles:
 * - File and URL input processing
 * - Transcription status management
 * - Error handling
 * - Processing time tracking
 * - Tab close protection
 *
 * The transcription process goes through several states:
 * 1. idle: Initial state
 * 2. uploading: File/URL is being uploaded
 * 3. processing: Audio is being transcribed
 * 4. completed: Transcription is finished
 * 5. error: An error occurred
 *
 * @example
 * ```tsx
 * function TranscriptionComponent() {
 *   const {
 *     status,
 *     transcription,
 *     handleFile,
 *     handleUrl,
 *     getProcessingTime
 *   } = useSpeechToText({
 *     onTranscriptionComplete: (result) => console.log(result),
 *     onError: (error) => console.error(error)
 *   });
 *
 *   return (
 *     <div>
 *       <input
 *         type="file"
 *         onChange={(e) => e.target.files?.[0] && handleFile(e.target.files[0])}
 *       />
 *       {status === 'processing' && (
 *         <p>Processing time: {getProcessingTime()}</p>
 *       )}
 *       {transcription && (
 *         <p>{transcription.text}</p>
 *       )}
 *     </div>
 *   );
 * }
 * ```
 *
 * @param callbacks - Object containing callback functions for transcription completion and errors
 * @returns Object containing transcription state and handler functions
 */
export function useSpeechToText({
  onTranscriptionComplete,
  onError,
  language = "id",
}: UseSpeechToTextCallbacks = {}): UseSpeechToTextReturn {
  const [state, setState] = useState<UseSpeechToTextState>({
    input: null,
    status: "idle",
    transcription: null,
    processingStartTime: null,
    transcriptionId: null,
  });

  const isActiveTranscription =
    state.status === "uploading" || state.status === "processing";

  // Add beforeunload event handler
  useEffect(() => {
    const handleBeforeUnload = (e: BeforeUnloadEvent) => {
      if (isActiveTranscription) {
        e.preventDefault();
        e.returnValue = "";
        return "You have an active transcription in progress. If you leave, the transcription will be lost. Are you sure you want to leave?";
      }
    };

    if (isActiveTranscription) {
      window.addEventListener("beforeunload", handleBeforeUnload);
      return () =>
        window.removeEventListener("beforeunload", handleBeforeUnload);
    }
  }, [isActiveTranscription]);

  const handleError = useCallback(
    (error: string) => {
      setState((prev) => ({
        ...prev,
        input: null,
        status: "error",
        transcription: { text: "", segments: [], error },
        processingStartTime: null,
        transcriptionId: null,
      }));
      onError?.(error);
    },
    [onError]
  );

  // Add polling effect when we have a transcriptionId
  useEffect(() => {
    if (!state.transcriptionId || state.status !== "processing") return;

    let isSubscribed = true;
    let attempts = 0;
    let currentInterval = POLLING_INTERVAL;

    const poll = async () => {
      try {
        const response = await fetch(
          `/api/transcription-status/${state.transcriptionId}`
        );

        if (!isSubscribed) return;

        if (response.ok) {
          const result = await response.json();
          setState((prev) => ({
            ...prev,
            status: "completed",
            transcription: result,
            processingStartTime: null,
            transcriptionId: null,
          }));
          onTranscriptionComplete?.(result);
          return true; // Signal to stop polling
        } else if (response.status !== 404) {
          // If we get any error other than 404 (not found), handle it
          const error = await response.json();
          throw new Error(error.message);
        }

        // Increase polling interval after first minute (30 attempts)
        attempts++;
        if (attempts > 30 && currentInterval < MAX_POLLING_INTERVAL) {
          currentInterval = Math.min(
            currentInterval * 1.5,
            MAX_POLLING_INTERVAL
          );
        }
        return false; // Continue polling
      } catch (error) {
        if (!isSubscribed) return true;
        handleError((error as Error).message);
        return true; // Stop polling on error
      }
    };

    const pollInterval = setInterval(async () => {
      const shouldStop = await poll();
      if (shouldStop) {
        clearInterval(pollInterval);
      }
    }, currentInterval);

    return () => {
      isSubscribed = false;
      clearInterval(pollInterval);
    };
  }, [
    state.transcriptionId,
    state.status,
    onTranscriptionComplete,
    handleError,
  ]);

  const getProcessingTime = useCallback(() => {
    if (!state.processingStartTime) return "";
    const minutes = Math.floor(
      (Date.now() - state.processingStartTime.getTime()) / 60000
    );
    return minutes > 0 ? `${minutes} minute${minutes > 1 ? "s" : ""}` : "";
  }, [state.processingStartTime]);

  const handleFile = useCallback(
    async (file: File) => {
      setState((prev) => ({
        ...prev,
        input: { type: "file", data: file },
        status: "uploading",
        transcription: null,
        processingStartTime: new Date(),
        transcriptionId: null,
      }));

      try {
        const result = (await transcribeAudio(
          file,
          (status) => {
            setState((prev) => ({ ...prev, status }));
          },
          language
        )) as TranscriptionResponse;

        // If we got a transcriptionId, update state for polling
        if (result.resultId) {
          setState((prev) => ({
            ...prev,
            status: "processing",
            transcriptionId: result.resultId || null,
          }));
          return;
        }

        // If we got immediate results, handle them
        setState((prev) => ({
          ...prev,
          status: "completed",
          transcription: result,
          processingStartTime: null,
        }));
        onTranscriptionComplete?.(result);
      } catch (error) {
        handleError((error as Error).message);
      }
    },
    [onTranscriptionComplete, handleError, language]
  );

  const handleUrl = useCallback(
    async (url: string) => {
      if (!url) {
        handleError("Please enter a URL");
        return;
      }

      try {
        new URL(url); // Validate URL format
      } catch {
        handleError("Please enter a valid URL");
        return;
      }

      setState((prev) => ({
        ...prev,
        input: { type: "url", data: url },
        status: "uploading",
        transcription: null,
        processingStartTime: new Date(),
        transcriptionId: null,
      }));

      try {
        const result = (await transcribeAudio(
          url,
          (status) => {
            setState((prev) => ({ ...prev, status }));
          },
          language
        )) as TranscriptionResponse;

        // If we got a transcriptionId, update state for polling
        if (result.resultId) {
          setState((prev) => ({
            ...prev,
            status: "processing",
            transcriptionId: result.resultId || null,
          }));
          return;
        }

        // If we got immediate results, handle them
        setState((prev) => ({
          ...prev,
          status: "completed",
          transcription: result,
          processingStartTime: null,
        }));
        onTranscriptionComplete?.(result);
      } catch (error) {
        handleError((error as Error).message);
      }
    },
    [onTranscriptionComplete, handleError, language]
  );

  return {
    input: state.input,
    status: state.status,
    transcription: state.transcription,
    handleFile,
    handleUrl,
    handleError,
    getProcessingTime,
    isActiveTranscription,
    transcriptionId: state.transcriptionId,
  };
}


================================================
File: src/lib/authUtils.ts
================================================
const AUTH_KEY = 'stt_auth_status';
const ACCESS_CODE = process.env.NEXT_PUBLIC_ACCESS_CODE;
if (!ACCESS_CODE) {
  throw new Error('NEXT_PUBLIC_ACCESS_CODE environment variable is required');
}

export const isAuthenticated = (): boolean => {
  if (typeof window === 'undefined') return false;
  return localStorage.getItem(AUTH_KEY) === 'true';
};

export const setAuthenticated = (status: boolean): void => {
  if (typeof window === 'undefined') return;
  if (status) {
    localStorage.setItem(AUTH_KEY, 'true');
  } else {
    localStorage.removeItem(AUTH_KEY);
  }
};

export const validateAccessCode = (code: string): boolean => {
  return code === ACCESS_CODE;
};

================================================
File: src/lib/constants.ts
================================================
export const ALLOWED_FORMATS = [
  // Audio formats
  "audio/mp3",
  "audio/mpeg",
  "audio/wav",
  "audio/wave",
  "audio/x-wav",
  "audio/flac",
  "audio/x-flac",
  "audio/aac",
  "audio/opus",
  "audio/ogg",
  "audio/vorbis",
  "audio/m4a",
  "audio/mpeg",
  "audio/x-m4a",
  "audio/mp4",
  "audio/x-mp4",
  // Video formats
  "video/mp4",
  "video/mpeg",
  "video/mov",
  "video/quicktime",
  "video/webm",
  // Generic formats that might be used
  "application/ogg",
] as const;

export type AllowedFormat = (typeof ALLOWED_FORMATS)[number];

export const FILE_LIMITS = {
  MAX_SIZE: 100 * 1024 * 1024, // 100MB for direct uploads
  URL_MAX_SIZE: 1024 * 1024 * 1024, // 1GB for URL-based uploads (according to Lemonfox docs)
} as const;

export const FILE_SIZE_ERROR = {
  OVER_LIMIT:
    "File size exceeds 100MB. Please use URL upload for larger files.",
  OVER_URL_LIMIT: "File size exceeds 1GB limit.",
  INVALID_FORMAT: (detectedType: string) =>
    `Invalid file format: "${detectedType}". Allowed formats: ${ALLOWED_FORMATS.join(
      ", "
    )}`,
} as const;


================================================
File: src/lib/transcriptionService.ts
================================================
import { TranscriptionResult } from "@/components/SpeechToText/types";

export type TranscriptionProgressCallback = (
  status: "uploading" | "processing"
) => void;

// Polling configuration
const POLLING_CONFIG = {
  INITIAL_INTERVAL: 2000, // 2 seconds
  MAX_INTERVAL: 10000, // 10 seconds
  INTERVAL_INCREASE_AFTER: 30, // Increase interval after 30 attempts (1 minute)
  MAX_DURATION: 3600000, // 1 hour in milliseconds
  MAX_ATTEMPTS: 720, // Maximum number of polling attempts
} as const;

export async function transcribeAudio(
  fileOrUrl: File | string,
  onProgress?: TranscriptionProgressCallback,
  language: string = "id" // Default to Indonesian
): Promise<TranscriptionResult> {
  const formData = new FormData();
  formData.append("file", fileOrUrl);
  formData.append("language", language);

  // Start the transcription
  onProgress?.("uploading");
  const response = await fetch("/api/transcribe", {
    method: "POST",
    body: formData,
  });

  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.message || "Failed to transcribe audio");
  }

  const result = await response.json();

  // If we got a resultId, it means we need to poll for results
  if (result.resultId) {
    onProgress?.("processing");
    return await pollForResults(result.resultId);
  }

  return result;
}

async function pollForResults(resultId: string): Promise<TranscriptionResult> {
  const startTime = Date.now();
  let interval: number = POLLING_CONFIG.INITIAL_INTERVAL;
  let attempts = 0;

  while (attempts < POLLING_CONFIG.MAX_ATTEMPTS) {
    try {
      const response = await fetch(`/api/transcription-status/${resultId}`);

      if (response.ok) {
        return await response.json();
      }

      if (response.status !== 404) {
        // If we get any error other than 404 (not found), throw it
        const error = await response.json();
        throw new Error(error.message);
      }

      // Check if we've exceeded the maximum duration
      if (Date.now() - startTime > POLLING_CONFIG.MAX_DURATION) {
        throw new Error(
          "Transcription timed out after 1 hour. Please try again or use a shorter audio file."
        );
      }

      // Gradually increase the interval after the initial period
      if (attempts > POLLING_CONFIG.INTERVAL_INCREASE_AFTER) {
        interval = Math.min(interval * 1.5, POLLING_CONFIG.MAX_INTERVAL);
      }

      // Wait before next attempt
      await new Promise((resolve) => setTimeout(resolve, interval));
      attempts++;
    } catch (error) {
      if (attempts === POLLING_CONFIG.MAX_ATTEMPTS - 1) {
        throw error;
      }
      // Continue polling on error unless it's the last attempt
    }
  }

  throw new Error(
    "Transcription timed out after 1 hour. Please try again or use a shorter audio file."
  );
}


================================================
File: src/lib/transcriptionStore.ts
================================================
import { TranscriptionResult } from "@/components/SpeechToText/types";

interface StoredTranscription {
  result: TranscriptionResult;
  timestamp: number;
  retrievedAt?: number;
}

/**
 * In-memory store for transcription results.
 * Note: In production, this should be replaced with a proper database.
 */
class TranscriptionStore {
  private results: Map<string, StoredTranscription>;
  private readonly CLEANUP_INTERVAL = 5 * 60 * 1000; // 5 minutes
  private readonly MAX_AGE = 60 * 60 * 1000; // 1 hour
  private readonly RETENTION_AFTER_RETRIEVAL = 60 * 1000; // 1 minute

  constructor() {
    this.results = new Map();
    this.startCleanupInterval();
  }

  private startCleanupInterval() {
    setInterval(() => {
      const now = Date.now();
      for (const [id, stored] of this.results.entries()) {
        // Remove if it's older than MAX_AGE
        if (now - stored.timestamp > this.MAX_AGE) {
          this.results.delete(id);
          continue;
        }

        // Remove if it's been retrieved and retention period has passed
        if (
          stored.retrievedAt &&
          now - stored.retrievedAt > this.RETENTION_AFTER_RETRIEVAL
        ) {
          this.results.delete(id);
        }
      }
    }, this.CLEANUP_INTERVAL);
  }

  /**
   * Store a transcription result
   * @returns The ID used to retrieve the result later
   */
  store(result: TranscriptionResult): string {
    const id = Date.now().toString();
    this.results.set(id, {
      result,
      timestamp: Date.now(),
    });
    return id;
  }

  /**
   * Get a transcription result by ID
   * @returns The transcription result or null if not found
   */
  get(id: string): TranscriptionResult | null {
    const stored = this.results.get(id);
    if (!stored) return null;

    // Mark as retrieved but don't delete immediately
    this.results.set(id, {
      ...stored,
      retrievedAt: Date.now(),
    });

    return stored.result;
  }

  /**
   * Check if a transcription result exists
   */
  has(id: string): boolean {
    return this.results.has(id);
  }
}

// Export a singleton instance
export const transcriptionStore = new TranscriptionStore();


